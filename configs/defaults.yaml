paths:
  outputs_root: outputs   # 训练/评估产物根目录（lora、processor、logs、figures、checkpoints）
  experiments_root: experiments  # 实验管理目录（新优化结构）

# Hydra配置：禁用自动输出目录管理
hydra:
  run:
    dir: .  # 禁用Hydra的自动输出目录

model:
  base_id: Qwen/Qwen2.5-VL-3B-Instruct

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  bias: "none"
  # 图像encoder部分使用LoRA训练
  # PEFT使用字符串包含匹配：匹配模块名称中包含指定字符串的Linear层
  # 根据模型结构检查（只匹配视觉encoder独有的层，避免匹配LLM部分）：
  # - qkv: 视觉encoder独有（LLM使用q_proj/k_proj/v_proj分开的）
  # - attn.proj: 视觉encoder的attention输出投影（LLM使用self_attn.o_proj，不会匹配）
  # 注意：gate_proj, up_proj, down_proj在视觉和LLM中都存在，暂时不训练MLP层避免冲突
  target_modules:
    - qkv       # 视觉encoder独有的qkv线性层（匹配model.visual.blocks.*.attn.qkv）
    - attn.proj # 视觉encoder的attention输出投影（匹配model.visual.blocks.*.attn.proj）
  # LLM主体部分的layer norm和输出头直接训练（不使用LoRA）
  # modules_to_save中的模块会被完整保存和训练，不使用LoRA
  # 根据模型结构检查：
  # - LLM的layer norm路径：model.language_model.layers.*.input_layernorm / post_attention_layernorm
  # - 视觉encoder使用norm1/norm2（Qwen2RMSNorm），不会冲突
  # - 顶层norm: model.language_model.norm
  modules_to_save:
    - lm_head                    # 输出头直接训练
    - input_layernorm            # LLM的input layer norm（只匹配LLM部分，视觉encoder使用norm1/norm2）
    - post_attention_layernorm  # LLM的post attention layer norm
    - language_model.norm       # LLM顶层norm（避免匹配到视觉encoder的norm）

data:
  name: FD001

  # === 新增：默认预处理设置 ===
  preprocess:
    raw_dir: data/raw                    # 原始 C-MAPSS 数据路径
    out_dir: data/processed              # 输出路径 (预处理结果 npy)
    rearley: 125                         # RUL 截断上限
    window_size: auto                    # FD001=30, FD002=20, FD003=30, FD004=15
    step: 1                              # 滑动步长
    test_last_window: true               # 测试集仅保留每台发动机最后一个窗
    scale_mode: minmax                   # Min–Max [-1,1]（论文通用）
    select14: false                      # 是否保留14个有效传感器（默认false）
  # ==================================

  # 训练/评估阶段读取图像数据的路径
  train_dir: data/images/FD001/train
  test_dir:  data/images/FD001/test
  image_glob: sample_*.png
  renderer: ${data.imgify.renderer}
  label_filename: y.npy
  image_size: 448
  seed: 42
  train_ratio: 0.8     # 若单目录切分时使用

  # === 图像化配置（mmts.cli.imgify） ===
  imgify:
    force_regenerate: true   # 是否强制重新生成图像（默认关闭）
    renderer: grayscale
    orientation: row-major
    flip_vertical: false
    flip_horizontal: false

    # === 默认使用预处理后的缩放结果 ===
    scale_mode: none          # 不再在 imgify 阶段做缩放（预处理已完成）
    pmin: 1
    pmax: 99
    patch_window: 20
    patch_stride: 1
    cmap: rainbow
    # =======================================

    train:
      x_npy: data/processed/FD001/X_train.npy
      y_npy: data/processed/FD001/y_train.npy
      out_dir: data/images/FD001/train
      max_samples: null

    test:
      x_npy: data/processed/FD001/X_test.npy
      y_npy: data/processed/FD001/y_test.npy
      out_dir: data/images/FD001/test
      max_samples: null
  # =======================================

train:
  epochs: 3
  batch_size: 8
  grad_accum: 4
  lr: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.05
  scheduler: cosine
  logging_steps: 10
  save_strategy: epoch
  bf16: true
  fp16: false
  grad_ckpt: true
  allow_tf32: true

eval:
  experiment_id: null  # 指定实验ID，为null时使用最新实验
  use_latest: true     # 是否使用最新实验
  
  # 生成超参
  max_new_tokens: 32
  do_sample: false
  num_beams: 1

  # RUL 合法区间（解析与裁剪）
  rul_low: 0.0
  rul_high: 125.0

  image_size: ${data.image_size}

  metrics:
    - rmse
    - mae
    - r2
    - valid_ratio

prompts:
  rules_file: configs/base_rules.txt
